import spacy
from spacy.training.example import Example
from spacy.util import minibatch, compounding
import json
import random
from pathlib import Path
from tqdm import tqdm

# -----------------------------
# –ü—É—Ç–∏
# -----------------------------
DATA_DIR = Path("data")                          # –ü–∞–ø–∫–∞ —Å —Ñ–∞–π–ª–∞–º–∏ .txt (JSON)
OUTPUT_MODEL_DIR = Path("parser/ner_model")      # –ü–∞–ø–∫–∞ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–∏

# -----------------------------
# –ó–∞–≥—Ä—É–∂–∞–µ–º –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å
# -----------------------------
# –ü–µ—Ä–µ–¥ –∑–∞–ø—É—Å–∫–æ–º: python -m spacy download ru_core_news_md
nlp = spacy.load("ru_core_news_md")

# -----------------------------
# –°–æ–∑–¥–∞—ë–º / –ø–æ–ª—É—á–∞–µ–º NER
# -----------------------------
if "ner" not in nlp.pipe_names:
    ner = nlp.add_pipe("ner", last=True)
else:
    ner = nlp.get_pipe("ner")

# -----------------------------
# –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
# -----------------------------
train_data = []
for file_path in sorted(DATA_DIR.glob("*.txt")):
    try:
        with open(file_path, "r", encoding="utf-8") as f:
            file_data = json.load(f)
    except json.JSONDecodeError:
        print(f"‚ö†Ô∏è –ü—Ä–æ–ø—É—â–µ–Ω —Ñ–∞–π–ª (–Ω–µ JSON): {file_path}")
        continue

    for item in file_data:
        text = item.get("text")
        entities = item.get("entities", [])
        if not text or not entities:
            continue
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç—å –¥–∏–∞–ø–∞–∑–æ–Ω–æ–≤
        valid_entities = []
        for start, end, label in entities:
            if 0 <= start < end <= len(text):
                valid_entities.append((start, end, label))
        if valid_entities:
            train_data.append((text, {"entities": valid_entities}))

print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(train_data)} –ø—Ä–∏–º–µ—Ä–æ–≤ –∏–∑ {len(list(DATA_DIR.glob('*.txt')))} —Ñ–∞–π–ª–æ–≤")

if not train_data:
    raise ValueError("‚ùå –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è. –ü—Ä–æ–≤–µ—Ä—å —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –ø–∞–ø–∫–∏ data/")

# -----------------------------
# –†–µ–≥–∏—Å—Ç—Ä–∏—Ä—É–µ–º –≤—Å–µ –ª–µ–π–±–ª—ã
# -----------------------------
for _, annotations in train_data:
    for ent in annotations.get("entities", []):
        ner.add_label(ent[2])

# -----------------------------
# –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
# -----------------------------
n_iterations = 10
dropout_rate = 0.3

# –û—Ç–∫–ª—é—á–∞–µ–º –æ—Å—Ç–∞–ª—å–Ω—ã–µ –ø–∞–π–ø—ã (—Ç–æ–ª—å–∫–æ NER)
other_pipes = [p for p in nlp.pipe_names if p != "ner"]
with nlp.disable_pipes(*other_pipes):
    optimizer = nlp.begin_training()

    for itn in range(1, n_iterations + 1):
        random.shuffle(train_data)
        losses = {}
        batches = minibatch(train_data, size=compounding(4.0, 32.0, 1.001))

        with tqdm(total=len(train_data), desc=f"–≠–ø–æ—Ö–∞ {itn}/{n_iterations}", ncols=100) as pbar:
            for batch in batches:
                examples = []
                for text, annots in batch:
                    doc = nlp.make_doc(text)
                    example = Example.from_dict(doc, annots)
                    examples.append(example)
                nlp.update(
                    examples,
                    sgd=optimizer,
                    drop=dropout_rate,
                    losses=losses,
                )
                pbar.update(len(batch))

        print(f"  üîπ –ü–æ—Ç–µ—Ä–∏ (loss): {losses.get('ner', 0):.4f}")

# -----------------------------
# –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å
# -----------------------------
OUTPUT_MODEL_DIR.mkdir(parents=True, exist_ok=True)
nlp.to_disk(OUTPUT_MODEL_DIR)
print(f"\n‚úÖ –ú–æ–¥–µ–ª—å –æ–±—É—á–µ–Ω–∞ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤: {OUTPUT_MODEL_DIR}")
